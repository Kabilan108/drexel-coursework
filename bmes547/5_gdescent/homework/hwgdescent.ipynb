{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "**Author:** [Tony Kabilan Okeke](mailto:tko35@drexel.edu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Iterations of Gradient Descent\n",
    "\n",
    "Let $f(x)=x^6 + 4*x^5 - 2*x^4 - 16*x^3 + 5*x^2 + 20*x - 12$\n",
    "\n",
    "Calculate the derivate of f(x) by hand or using Matlab. You are allowed to \n",
    "hard-code the derivative of f(x). Plot this function and its derivative on the \n",
    "same figure, for x=-3.1 ... 1.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x_0=-1.5$ and learning rate $\\eta=0.1$. Calculate and print x1, the next \n",
    "value of x, after 1 iteration of the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and print x2, the next value of x, after another iteration of the \n",
    "gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many iterations of Gradient Descent\n",
    "\n",
    "Using the same function f(x) above, calculate and store $x_0,x_1,x_2,...,x_{10}$ \n",
    "into a variable called `xhistory`. You must use a for loop to code this. Do not \n",
    "write separate code for calculation of x0, calculation of x1, etc.\n",
    "\n",
    "Display a history plot of x, (horizontal should show the iterations 1..10 and \n",
    "vertical axis should show the values that x has at each of those iterations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement & test your own gdescent optimization function.\n",
    "\n",
    "Write a function `[finalx, finalf] = gdescent(f,fprime,x0,eta,maxiters)` that \n",
    "finds the local minimum of a function f whose derivative is given as fprime. \n",
    "Use the input argument x0 as the initial \"guess\" and improve this initial guess \n",
    "using gradient descent algorithm, with learning rate eta and for maxiters \n",
    "iterations. The function should return the final improved value of x and the \n",
    "value of the function at that x. Inside the function, also plot the history of \n",
    "the x values that your algorithm improves upon, until it reaches the final guess. \n",
    "Assume that f is a function of a single scalar variable (ie. 1-dimensional). \n",
    "You may write the gdescent function in a separate file if convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdescent(f, fprime, x0, eta, maxiters) -> tuple:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test1\n",
    "\n",
    "Test your gdescent implementation with the function f given above. \n",
    "\n",
    "- Use $x_0=-1.5,~\\eta=0.1$, and `maxiters=50`. Display the finalx and finalf \n",
    "  values that gdescent() returns.\n",
    "- Using $x_0=-1.5$ and `maxiters=50`, identify and use a value of eta that \n",
    "  ensures convergence to the local minima within that many iterations. \n",
    "  - You may use trial and error to identify an eta value, but only show here \n",
    "    the final eta that you came up with. Display the finalx and finalf values that gdescent() returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test2\n",
    "\n",
    "Do not change the code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: 2*np.sin(x) - 3*np.cos(x) + x\n",
    "fprime = lambda x: 2*np.cos(x) + 3*np.sin(x) + 1\n",
    "finalx, finalf = gdescent(f, fprime, 0, 0.1, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmes547",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
